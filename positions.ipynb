{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#World\n",
    "no_of_time_blocks=8\n",
    "\n",
    "#states - time blocks\n",
    "S=[s for s in range(no_of_time_blocks)]\n",
    "terminal = no_of_time_blocks-1\n",
    "\n",
    "#actions\n",
    "A=['same','other']\n",
    "a_count=len(A)\n",
    "\n",
    "#discounting\n",
    "alpha=0.1\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Agent:\n",
    "    epsilon = 0.1\n",
    "    def __init__(self,S,A,alpha,gamma):\n",
    "        self.Q = {s: {a: 0 for a in A} for s in S}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update_Q(self,s,sn,a,r):\n",
    "        if self.Q[sn]['same']>=self.Q[sn]['other']:\n",
    "            max_q = self.Q[sn]['same']\n",
    "        else:\n",
    "            max_q = self.Q[sn]['other']\n",
    "        self.Q[s][a]=self.Q[s][a]+alpha*(r+gamma*max_q-self.Q[s][a])\n",
    "\n",
    "    def action(self, s):\n",
    "        c=random.random()\n",
    "        if self.Q[s]['same']>=self.Q[s]['other']:\n",
    "            if c>=Agent.epsilon:\n",
    "                return 'same'\n",
    "            else:\n",
    "                return 'other'\n",
    "        else:\n",
    "            if c>=Agent.epsilon:\n",
    "                return 'other'\n",
    "            else:\n",
    "                return 'same'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class ConstraintSystem:\n",
    "    def __init__(self, no_of_agents, no_of_people):\n",
    "        self.prev_action=[None for i in range(no_of_agents)]\n",
    "        self.no_of_agents=no_of_agents\n",
    "        self.no_of_people=no_of_people\n",
    "        self.no_of_free=no_of_people-no_of_agents\n",
    "\n",
    "    def checkActions(self, agent_actions):\n",
    "        consys_actions=copy.deepcopy(agent_actions)\n",
    "        penalise=[]\n",
    "        count_extra = self.no_of_free - self.prev_action.count('same')\n",
    "        for i in range(self.no_of_agents):\n",
    "            if self.prev_action[i]!=agent_actions[i]:\n",
    "                penalise.append(0)\n",
    "            elif agent_actions[i]=='same': #will lead to overwork\n",
    "                penalise.append(-100)\n",
    "                consys_actions[i]='other'\n",
    "            elif count_extra>0:\n",
    "                count_extra -= 1\n",
    "                penalise.append(0)\n",
    "            else:\n",
    "                penalise.append(-100) #will lead to no one being scheduled\n",
    "                consys_actions[i]='same'\n",
    "        self.prev_action=copy.deepcopy(consys_actions)\n",
    "        return consys_actions, penalise\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define environment\n",
    "from queue import Queue, Empty\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self,no_of_positions,no_of_people):\n",
    "        self.table={p: {s: None for s in S} for p in range(no_of_positions)}\n",
    "\n",
    "        self.rest= Queue()\n",
    "        #in final code, replace the below block with actual people names\n",
    "        for person in range(no_of_people):\n",
    "            self.rest.put(person)\n",
    "        for i in self.table:\n",
    "            self.table[i][0] = self.rest.get()\n",
    "\n",
    "    def schedule(self,actions,s,reward):\n",
    "        sn=s+1\n",
    "        removed=[]\n",
    "        for p in self.table:\n",
    "            if actions[p] == 'same':\n",
    "                reward[p] += 1\n",
    "                self.table[p][sn]=self.table[p][s]\n",
    "            else:\n",
    "                if self.rest.empty():\n",
    "                    reward[p]-=100\n",
    "                    #print('oops')\n",
    "                    return reward, True\n",
    "                removed.append(self.table[p][s])\n",
    "                self.table[p][sn]=self.rest.get()\n",
    "                reward[p]+=1\n",
    "        for ppl in removed:\n",
    "            self.rest.put(ppl)\n",
    "        return reward, False\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t4\t4\t2\t2\t0\t0\t4\t\n",
      "1\t5\t5\t3\t3\t1\t1\t5\t\n",
      "2\t2\t0\t0\t4\t4\t2\t2\t\n",
      "3\t3\t1\t1\t5\t5\t3\t3\t\n"
     ]
    }
   ],
   "source": [
    "#Initalize no of episodes\n",
    "no_of_episodes=100\n",
    "\n",
    "#initialize agents - aka positions\n",
    "no_of_agents=4\n",
    "agents = [Agent(S,A,alpha,gamma) for i in range(no_of_agents)]\n",
    "Agent.epsilon=0.1\n",
    "#initalize min no of ppl\n",
    "x=no_of_agents\n",
    "min_no_of_people=x//2+(x-x//2)*2\n",
    "\n",
    "#check actual amount of ppl\n",
    "no_of_people=x\n",
    "if no_of_people < min_no_of_people:\n",
    "    #in final code, append a bunch of \"attention, out of constraint\" or smtg blocks to the people names\n",
    "    no_of_people= min_no_of_people\n",
    "no_of_people=min_no_of_people\n",
    "#no_of_people=6\n",
    "\n",
    "#initialize constraints\n",
    "constraints = ConstraintSystem(no_of_agents,no_of_people)\n",
    "\n",
    "for e in range(no_of_episodes):\n",
    "    env=Environment(no_of_agents,no_of_people)\n",
    "    for s in S[:terminal]:\n",
    "        agent_actions=[]\n",
    "        for agent in agents:\n",
    "            agent_actions.append(agent.action(s))\n",
    "        consys_actions, reward = constraints.checkActions(agent_actions)\n",
    "        reward,restart = env.schedule(consys_actions,s,reward)\n",
    "        i=0\n",
    "        for agent in agents:\n",
    "            agent.update_Q(s,s+1,agent_actions[i],reward[i])\n",
    "            i=i+1\n",
    "        if restart:\n",
    "            break\n",
    "\n",
    "for i in range(no_of_agents):\n",
    "    for j in env.table[i]:\n",
    "        print(env.table[i][j], end='\\t')\n",
    "    print()\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t2\t2\t4\t4\t0\t0\t\n",
      "1\t1\t3\t3\t5\t5\t1\t1\t\n",
      "2\t4\t4\t0\t0\t2\t2\t4\t\n",
      "3\t5\t5\t1\t1\t3\t3\t5\t\n"
     ]
    }
   ],
   "source": [
    "#testing with epsilon 0\n",
    "Agent.epsilon=0\n",
    "env=Environment(no_of_agents,no_of_people)\n",
    "for s in S[:terminal]:\n",
    "    agent_actions=[]\n",
    "    for agent in agents:\n",
    "        agent_actions.append(agent.action(s))\n",
    "    consys_actions, reward = constraints.checkActions(agent_actions)\n",
    "    reward,restart = env.schedule(consys_actions,s,reward)\n",
    "    i=0\n",
    "    for agent in agents:\n",
    "        agent.update_Q(s,s+1,agent_actions[i],reward[i])\n",
    "        i=i+1\n",
    "    if restart:\n",
    "        break\n",
    "\n",
    "for i in range(no_of_agents):\n",
    "    for j in env.table[i]:\n",
    "        print(env.table[i][j], end='\\t')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
